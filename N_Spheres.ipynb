{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import gamma\n",
        "from scipy import pi\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter"
      ],
      "metadata": {
        "id": "jqqP8_0EvrM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqf2SjlJFkBg",
        "outputId": "48dcaa52-f542-417d-d593-9889cc0670f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.36986495  0.59166125  1.33863525  1.87152487  3.98555614]\n",
            "initi 800\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "(2,)\n",
            "1142\n",
            "556\n",
            "[[389.10210216 753.82835235]\n",
            " [393.30088323 772.92250437]\n",
            " [378.76207569 750.87647825]\n",
            " ...\n",
            " [ 68.55336636 188.07174072]\n",
            " [149.81880414 193.52554167]\n",
            " [185.89877681 194.51222316]]\n",
            "[[528.11398303 229.20687516]\n",
            " [938.04717689 752.84757303]\n",
            " [317.08572643 368.58346296]\n",
            " ...\n",
            " [750.0504537   53.94304331]\n",
            " [172.23844552 610.91339085]\n",
            " [486.97028088  63.25860423]]\n",
            "[[389.10210216 753.82835235]\n",
            " [393.30088323 772.92250437]\n",
            " [378.76207569 750.87647825]\n",
            " ...\n",
            " [ 68.55336636 188.07174072]\n",
            " [149.81880414 193.52554167]\n",
            " [185.89877681 194.51222316]]\n",
            "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
            "(1142, 2)\n",
            "(556, 2)\n",
            "[389.10210216 753.82835235]\n",
            "[528.11398303 229.20687516]\n",
            "               1           2  label\n",
            "0     389.102102  753.828352   True\n",
            "1     393.300883  772.922504   True\n",
            "2     378.762076  750.876478   True\n",
            "3     404.716122  754.004851   True\n",
            "4     390.446236  750.594037   True\n",
            "...          ...         ...    ...\n",
            "1693  995.074962  662.388912  False\n",
            "1694  231.300654  872.405810  False\n",
            "1695  750.050454   53.943043  False\n",
            "1696  172.238446  610.913391  False\n",
            "1697  486.970281   63.258604  False\n",
            "\n",
            "[1698 rows x 3 columns]\n",
            "               1           2  label\n",
            "821   123.690793  865.439784   True\n",
            "265   530.144968  106.960433   True\n",
            "236   580.480808   -2.607171   True\n",
            "14    829.301712  422.157880   True\n",
            "621   532.622279  470.253581   True\n",
            "...          ...         ...    ...\n",
            "1539  307.402420  842.472351  False\n",
            "1287  747.306061   81.006373  False\n",
            "644   411.085571  438.819294   True\n",
            "1220  838.256477  276.776856  False\n",
            "1081  143.792331  173.381827   True\n",
            "\n",
            "[1698 rows x 3 columns]\n",
            "               1           2  label\n",
            "821   123.690793  865.439784   True\n",
            "265   530.144968  106.960433   True\n",
            "236   580.480808   -2.607171   True\n",
            "14    829.301712  422.157880   True\n",
            "621   532.622279  470.253581   True\n",
            "...          ...         ...    ...\n",
            "1539  307.402420  842.472351  False\n",
            "1287  747.306061   81.006373  False\n",
            "644   411.085571  438.819294   True\n",
            "1220  838.256477  276.776856  False\n",
            "1081  143.792331  173.381827   True\n",
            "\n",
            "[1698 rows x 3 columns]\n",
            "ratio  0.6725559481743227\n"
          ]
        }
      ],
      "source": [
        "def remove_array(L,arr):\n",
        "    ind = 0\n",
        "    size = len(L)\n",
        "    while ind != size and not np.array_equal(L[ind],arr):\n",
        "        ind += 1\n",
        "    if ind != size:\n",
        "        L.pop(ind)\n",
        "    else:\n",
        "        raise ValueError('array not found in list.')\n",
        "def make_point_on_sphere(dimensions, radius, offset):\n",
        "  while True:\n",
        "    r = np.random.uniform(low=0, high=radius, size=dimensions)\n",
        "    if (np.power(r, 2).sum() < radius + 0.01):\n",
        "      signs = np.random.choice([-1, 1], size=dimensions)\n",
        "      r = r * signs\n",
        "      r = r + offset \n",
        "      return r\n",
        "def make_multiple_points_on_sphere(dimensions, radius, offset, number, space): #START FIXING HERE\n",
        "  point_list = [make_point_on_sphere(dimensions, radius, offset) for i in range(number)]\n",
        "  additional_point_list = [make_point_on_sphere(dimensions, np.random.uniform(low=0.0, high=radius), offset) for i in range(number)]\n",
        "  random_points_inside = []\n",
        "  for point in space:\n",
        "    if np.sum(np.power(point - offset, [2 for dim in range(dimensions)])) <= (radius + 0.1*radius):\n",
        "      remove_array(space, point)\n",
        "  point_list.extend(additional_point_list)\n",
        "  return point_list, space\n",
        "def place_spheres(dimensions, types_of_sphere: int, min_radius, max_radius, min_offset, max_offset, min_no_of_points, max_no_of_points, space):\n",
        "  point_list = []\n",
        "  for i in range(types_of_sphere):\n",
        "    if (min_no_of_points == max_no_of_points):\n",
        "      no_points = int(min_no_of_points)\n",
        "    else:\n",
        "      no_points = np.random.randint(low=min_no_of_points, high=max_no_of_points)\n",
        "    radius = np.random.uniform(low=min_radius, high=max_radius)\n",
        "    offset = np.random.uniform(low=min_offset, high=max_offset, size=dimensions)\n",
        "    points, space = make_multiple_points_on_sphere(dimensions, radius, offset, no_points, space)\n",
        "    point_list.extend(points)\n",
        "  return point_list, space\n",
        "def get_spheres_binary(dimensions, types_of_sphere: int, min_radius, max_radius, min_offset, max_offset, min_no_of_points, max_no_of_points, label_not_sphere, label_sphere, no_of_random_points, minimum, maximum):\n",
        "  min_radius = min_radius ** 2 #necessary correction\n",
        "  max_radius = max_radius ** 2 #necessary correction\n",
        "  points = [np.random.uniform(low=minimum, high=maximum, size=dimensions) for i in range(no_of_random_points)]\n",
        "  dataset, points = place_spheres(dimensions, types_of_sphere, min_radius, max_radius, min_offset, max_offset, min_no_of_points, max_no_of_points, points)\n",
        "  dataset = np.stack(dataset, axis=0)\n",
        "  points = np.array(points)\n",
        "  dataset_labels = [label_sphere for i in range(len(dataset))]\n",
        "  points_labels = [label_not_sphere for i in range(len(points))]\n",
        "  data = np.concatenate((dataset, points), axis=0)\n",
        "  labels = dataset_labels + points_labels\n",
        "  df = pd.DataFrame(data, columns=[i+1 for i in range(dimensions)])\n",
        "  df['label'] = labels\n",
        "  df = df.sample(frac=1)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bK-60k2m6IP"
      },
      "outputs": [],
      "source": [
        "# Functions\n",
        "def add_neuron(model, chosen_layer: int):\n",
        "  model.layers[chosen_layer].units += 1\n",
        "  n_model = model_from_json(model.to_json())\n",
        "  return n_model\n",
        "def remove_neuron(model, chosen_layer: int):\n",
        "  model.layers[chosen_layer].units -= 1\n",
        "  n_model = model_from_json(model.to_json())\n",
        "  return n_model\n",
        "def add_layer(model, position, neurons):\n",
        "  '''\n",
        "  Adds a relu layer to a keras.Sequential model. Note: The implementation is a bit hacky and I am not satisfied with it.\n",
        "  Try to come up with other solutions\n",
        "  '''\n",
        "  layer_list = [layr.units for layr in model.layers]\n",
        "  #take the TensorFlow dimension, flips it, removes the None values, changes it into a tuple. This is done as in not to result in a multiple output shapes error (underdefined model)\n",
        "  input = tuple(filter(lambda item: item is not None, np.flip(model.input.shape)))\n",
        "  last_layer = layers.Dense(1, activation='sigmoid') # This is optimised for binary classification problems\n",
        "  layer_list.insert(position, neurons)\n",
        "  layer_list.pop()\n",
        "  n_model = keras.Sequential()\n",
        "  n_model.add(layers.Input(shape=input))\n",
        "  for neur in layer_list:\n",
        "    n_model.add(layers.Dense(units=neur, activation='relu'))\n",
        "  n_model.add(last_layer)\n",
        "  return n_model\n",
        "def remove_layer(model, position):\n",
        "  '''\n",
        "  works almost identically as add layer\n",
        "  '''\n",
        "  layer_list = [layr.units for layr in model.layers]\n",
        "  #take the TensorFlow dimension, flips it, removes the None values, changes it into a tuple. This is done as in not to result in a multiple output shapes error (underdefined model)\n",
        "  input = tuple(filter(lambda item: item is not None, np.flip(model.input.shape)))\n",
        "  last_layer = layers.Dense(1, activation='sigmoid') # This is optimised for binary classification problems\n",
        "  layer_list.pop(position)\n",
        "  layer_list.pop()\n",
        "  n_model = keras.Sequential()\n",
        "  n_model.add(layers.Input(shape=input))\n",
        "  for layr in layer_list:\n",
        "    n_model.add(layers.Dense(units=layr, activation='relu'))\n",
        "  n_model.add(last_layer)\n",
        "  return n_model  \n",
        "def remove_layer_experimental(model, position, neurons):\n",
        "  '''\n",
        "  I like this version of remove layer more. It's more versatile. However, I can see it causing plenty of bugs.\n",
        "  '''\n",
        "  layer_list = model.layers\n",
        "  #take the TensorFlow dimension, flips it, removes the None values, changes it into a tuple. This is done as in not to result in a multiple output shapes error (underdefined model)\n",
        "  input = tuple(filter(lambda item: item is not None, np.flip(model.input.shape)))\n",
        "  layer_list.pop(position)\n",
        "  n_model = keras.Sequential()\n",
        "  n_model.add(layers.Input(shape=input))\n",
        "  for layer in layer_list:\n",
        "    n_model.add(layer)\n",
        "  return n_model\n",
        "def calculate_neuron_acceptance_probability( NN:float, energy_N:float, energy_N_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = 1/(NN+1)\n",
        "  z = np.exp(-beta*nu)\n",
        "  exponent  = np.exp(-beta*(energy_N_1-energy_N))\n",
        "  probability = coeff * exponent * z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability \n",
        "def calculate_neuron_removal_probability(NN:float, energy_N:float, energy_N_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = NN\n",
        "  inverse_z = np.exp(beta*nu)\n",
        "  exponent  = np.exp(-beta*(energy_N-energy_N_1))\n",
        "  probability = coeff * exponent * inverse_z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability \n",
        "def calculate_layer_acceptance_probability(NL: float, M:float, energy_NL:float, energy_NL_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = (NL*M) / (NL+1)\n",
        "  z = np.exp(-beta*nu*M)\n",
        "  exponent = np.exp(-beta*(energy_NL_1-energy_NL))\n",
        "  probability = coeff * exponent * z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability\n",
        "def calculate_layer_removal_probability(NL: float, M:float, energy_NL:float, energy_NL_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = NL / (M*(NL-1))\n",
        "  inverse_z = np.exp(beta*nu*M)\n",
        "  exponent = np.exp(-beta*(energy_NL-energy_NL_1))\n",
        "  probability = coeff * exponent * inverse_z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability\n",
        "def construct_model_binary_classification(input_shape: tuple, layer_list: list):\n",
        "  '''\n",
        "  Adapt the last layer to anything\n",
        "  '''\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "  for neurons in layer_list: \n",
        "    model.add(keras.layers.Dense(neurons, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "def calculate_GC(model, loss:  float, nu: float) -> \"float\":  \n",
        "  return loss + nu*model.count_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xecdAZy4yIe"
      },
      "outputs": [],
      "source": [
        "# run simulation functions\n",
        "# FOR DEBUGGING PURPOSES ONLY\n",
        "probability_list = []\n",
        "loss_list = []\n",
        "accuracy_list = []\n",
        "layer_list = []\n",
        "model_evolution_list = []\n",
        "neuron_total_list = []\n",
        "layer_list = []\n",
        "neuron_layer_list = []\n",
        "# FOR DEBUGGING PURPOSES ONLY/end\n",
        "#REMEMBER TO ADD ANOTATIONS TO THE NEW SAVING METHODS \n",
        "def compare_losses(model, saved_model_labels, x_test, y_test, repo):\n",
        "  #could probably be made more efficient by also saving the losses?\n",
        "  model_loss, model_accuracy = model.evaluate(x_test, y_test)\n",
        "  if(len(saved_model_labels)<5):\n",
        "    repo += str(time.time())\n",
        "    model.save(repo)\n",
        "    return\n",
        "  for model_challenged_label in saved_model_labels:\n",
        "    model_challenged = load_model(repo + str(model_challenged_label))\n",
        "    #what is more efficient? writing a new code as a function or just rolling with it? I mean in all honesty the function would have to be changed with every new configuration so...same thing?\n",
        "    model_challenged_loss, model_challenged_accuracy = model_challenged.evaluate(x_test, y_test)\n",
        "    if model_loss < model_challenged_loss :\n",
        "      shutil.rmtree(repo + model_challenged_label)\n",
        "      repo += str(time.time())\n",
        "      model.save(repo)\n",
        "      #add removal of worst model, somehow\n",
        "\n",
        "      break\n",
        "  return\n",
        "def save_best_model(model, x_test, y_test, repo):\n",
        "  saved_model_labels = []\n",
        "  for filename in os.listdir(repo):\n",
        "    saved_model_labels.append(filename)\n",
        "  compare_losses(model, saved_model_labels, x_test, y_test, repo)\n",
        "  return\n",
        "def save_model(model, x_train, y_train, x_test, y_test):\n",
        "  #ADDED THE DATA FOR DEBUGING\n",
        "  #ADDED THIS FOR DEBUGGING AND ANALISING DATA\n",
        "  _layers = model.layers\n",
        "  _layers_neurons = [_layer.units for _layer in _layers]\n",
        "  _neurons_with_layers = list(zip(_layers_neurons, _layers))\n",
        "  neurons = sum(_layers_neurons)\n",
        "  layer_list.append(len(_layers))\n",
        "  neuron_total_list.append(neurons)\n",
        "  neuron_layer_list.append(_neurons_with_layers)\n",
        "  #END THINGS USED FOR PLOTTING\n",
        "  name = folderName\n",
        "  save_best_model(model, x_test, y_test, name)\n",
        "  name += str(time.time())\n",
        "  return\n",
        "def run_model(model, x_train, y_train, x_test, y_test):\n",
        "  #REMEMBER TO MAKE BOTH IDENTICAL AT ALL TIMES\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "  model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=15,verbose=0)\n",
        "  loss , accuracy = model.evaluate(x_test, y_test)\n",
        "  return loss\n",
        "def run_model_DEBUG(model, x_train, y_train, x_test, y_test):\n",
        "  #REMEMBER TO MAKE BOTH IDENTICAL AT ALL TIMES\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "  model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=15,verbose=0)\n",
        "  loss , accuracy = model.evaluate(x_test, y_test)\n",
        "  return loss, accuracy\n",
        "\n",
        "def run_sim_neuron(model, loss, x_train, y_train, x_test, y_test, beta, nu):\n",
        "  '''\n",
        "  function that runs move 1. loss was the term chosen for the energy of the system before the move, energy was chosen for the energy fo the system after the move.\n",
        "  even if they are technically the same in terms of units (both losses/energy)\n",
        "  this was chosen to avoid confusion between the two\n",
        "  '''\n",
        "  #brute force functions to solve the neuron removal problem\n",
        "  check_neurons_list = [lay.units <= 1 for lay in model.layers[1:-1]]\n",
        "  if all(check_neurons_list) == True:\n",
        "    return model\n",
        "  while True: \n",
        "    if(len(model.layers) == 2): \n",
        "      NN = 1\n",
        "    else:\n",
        "      layer = np.random.randint(low=1, high=len(model.layers)-1)\n",
        "    NN = model.layers[layer].units\n",
        "    if NN > 1: \n",
        "      break\n",
        "  #END BRUTE FORCE\n",
        "  add_neuron_prob = np.random.choice([True, False])\n",
        "  if add_neuron_prob == True: \n",
        "    new_model = add_neuron(model, layer)\n",
        "  else:\n",
        "    new_model = remove_neuron(model, layer)\n",
        "  #calculate model loss \n",
        "  energy, accuracy = run_model_DEBUG(new_model, x_train, y_train, x_test, y_test) # ADDED ACCURACY FOR DEBUGGING\n",
        "  #run acceptance\n",
        "  if(add_neuron_prob == True):\n",
        "    probability = calculate_neuron_acceptance_probability(NN, loss, energy, beta, nu)\n",
        "  else:\n",
        "    probability = calculate_neuron_removal_probability(NN-1, energy, loss, beta, nu)\n",
        "    # it is assumed that for this NN+1(i) is the model before the operation, and NN(i) is the model after the operation, hence the NN-1, and the reversed orders of energy and loss.\n",
        "  if np.random.rand() < probability:\n",
        "    probability_list.append(probability) #FOR DEBUGGING ONLY \n",
        "    #just for trial, we save every model we ever make for this time\n",
        "    save_model(new_model, x_train, y_train, x_test, y_test)\n",
        "    loss_list.append(energy)\n",
        "    accuracy_list.append(accuracy)\n",
        "    model_evolution_list.append(energy)\n",
        "\n",
        "    return new_model\n",
        "  else:\n",
        "    model_evolution_list.append(loss) #FOR DEBUGGING, both\n",
        "    return model\n",
        "\n",
        "def run_sim_layer(model, loss, x_train, y_train, x_test, y_test, beta, nu, M):\n",
        "  '''\n",
        "  function that runs move 1. loss was the term chosen for the energy of the system before the move, energy was chosen for the energy fo the system after the move.\n",
        "  even if they are technically the same in terms of units (both losses/energy)\n",
        "  this was chosen to avoid confusion between the two\n",
        "  '''\n",
        "  neurons = np.random.randint(low=1, high=M)\n",
        "  add_layer_prob = np.random.choice([True, False])\n",
        "  if add_layer_prob == True: \n",
        "    layer = np.random.randint(low=1, high=len(model.layers)-1)\n",
        "    new_model = add_layer(model, layer, neurons)\n",
        "  else:\n",
        "    #brute force way\n",
        "    if(len(model.layers)-2 <= 1):\n",
        "      return model\n",
        "    layer = np.random.randint(low=1, high=len(model.layers)-2)\n",
        "    removed_neurons = model.layers[layer].units\n",
        "    new_model = remove_layer(model, layer)\n",
        "  #calculate model loss \n",
        "  NL = len(new_model.layers)\n",
        "  energy, accuracy = run_model_DEBUG(new_model, x_train, y_train, x_test, y_test) #ADDED ACCURACY FOR DEBUGGING\n",
        "  #run acceptance\n",
        "  if(add_layer_prob == True):\n",
        "    probability = calculate_layer_acceptance_probability(NL, neurons, loss, energy, beta, nu)\n",
        "  else:\n",
        "    probability = calculate_layer_removal_probability(NL, removed_neurons, energy, loss, beta, nu)\n",
        "    # it is assumed that for this NN+1(i) is the model before the operation, and NN(i) is the model after the operation, hence the NN-1, and the reversed orders of energy and loss.\n",
        "  if np.random.rand() < probability:\n",
        "    probability_list.append(probability) #FOR DEBUGGING PURPOSES ONLY\n",
        "    #just for trial, we save every model we ever make for this time\n",
        "    save_model(new_model, x_train, y_train, x_test, y_test)\n",
        "    loss_list.append(energy)\n",
        "    accuracy_list.append(accuracy)\n",
        "    model_evolution_list.append(energy)\n",
        "    return new_model\n",
        "  else:\n",
        "    model_evolution_list.append(loss)\n",
        "    return model\n",
        "\n",
        "def run_sim_step(model, x_train, y_train, x_test, y_test, f, beta, nu, M):\n",
        "  loss = run_model(model, x_train, y_train, x_test, y_test)\n",
        "  if np.random.rand() < f:\n",
        "    model = run_sim_neuron(model, loss, x_train, y_train, x_test, y_test, beta, nu)\n",
        "  else:\n",
        "    model = run_sim_layer(model, loss, x_train, y_train, x_test, y_test, beta, nu, M)\n",
        "  return model \n",
        "def run_sim_for_loop(x_train, y_train, x_test, y_test, f, beta, nu, M, number_of_starting_layers, low_neuron, high_neuron, iterations):\n",
        "  starting_layers = np.random.randint(low=2, high=number_of_starting_layers)\n",
        "  if(low_neuron == high_neuron):\n",
        "    model_layers = [low_neuron for i in range(starting_layers)]\n",
        "  else:\n",
        "    model_layers = np.random.randint(low=low_neuron, high=high_neuron, size=starting_layers)\n",
        "  model = construct_model_binary_classification(x_train.shape[1], model_layers)\n",
        "  loss = run_model(model, x_train, y_train, x_test, y_test)\n",
        "  save_model(model, x_train, y_train, x_test, y_test)\n",
        "  energy, accuracy = run_model_DEBUG(model, x_train, y_train, x_test, y_test)\n",
        "  accuracy_list.append(accuracy)\n",
        "  model_evolution_list.append(energy)\n",
        "  for i in range(iterations):\n",
        "    model = run_sim_step(model, x_train, y_train, x_test, y_test, f, beta, nu, M)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTOQPhsy6mR1"
      },
      "outputs": [],
      "source": [
        "# for debugging purposes \n",
        "def run_sim_for_loop_loss_included(x_train, y_train, x_test, y_test, f, beta, nu, M, number_of_starting_layers, low_neuron, high_neuron, iterations, loss_list=[], mae_list=[]):\n",
        "  starting_layers = np.random.randint(low=2, high=number_of_starting_layers)\n",
        "  if(low_neuron == high_neuron):\n",
        "    model_layers = [low_neuron for i in range(starting_layers)]\n",
        "  else:\n",
        "    model_layers = np.random.randint(low=low_neuron, high=high_neuron, size=starting_layers)\n",
        "  model = construct_model_binary_classification(x_train.shape[1], model_layers)\n",
        "  loss = run_model(model, x_train, y_train, x_test, y_test)\n",
        "  save_model(model, x_train, y_train, x_test, y_test)\n",
        "  loss_list.append(loss)\n",
        "  energy, accuracy = run_model_DEBUG(model, x_train, y_train, x_test, y_test)\n",
        "  accuracy_list.append(accuracy)\n",
        "  model_evolution_list.append(energy)\n",
        "  for i in range(iterations):\n",
        "    model = run_sim_step(model, x_train, y_train, x_test, y_test, f, beta, nu, M)\n",
        "    #loss, mae = model.evaluate(x_test, y_test)\n",
        "    #loss_list.append(loss)\n",
        "    #mae_list.append(mae)\n",
        "  return model # loss_list, mae_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BrAWM1pR3pV",
        "outputId": "b9e9284d-6cfb-4403-e8dd-13c6fdebe790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5000000000000002\n"
          ]
        }
      ],
      "source": [
        "#functions for right packing\n",
        "def calculate_relative_volume(dimensions, radius, lower_volume_bound, upper_volume_bound):\n",
        "  v_sphere = ((pi ** (dimensions/2)) / (gamma(dimensions/2 + 1))) * radius ** dimensions\n",
        "  total_volume = (upper_volume_bound-lower_volume_bound) ** dimensions\n",
        "  relative_volume = v_sphere / total_volume\n",
        "  return relative_volume\n",
        "def calculate_sphere_volume(dimensions, radius):\n",
        "  v_sphere = ((pi ** (dimensions/2)) / (gamma((dimensions/2) + 1))) * radius ** dimensions\n",
        "  return v_sphere\n",
        "def calculate_radius(dimensions, packing_ratio, number_of_spheres, lower_volume_bound, upper_volume_bound):\n",
        "  v_sphere = (packing_ratio * ((upper_volume_bound - lower_volume_bound) ** dimensions)) / number_of_spheres\n",
        "  radius = ((v_sphere * gamma((dimensions/2) + 1)) / (pi ** (dimensions/2))) ** (1/dimensions)\n",
        "  return radius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akgz4pjjFF1C",
        "outputId": "a5b172de-7d3e-4848-8257-c83234f37b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5; python_version >= \"3.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-python) (1.23.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# long simulatiob functions\n",
        "\n",
        "def long_simulation(dimensions, number_spheres, length_of_space, nu, pf, folder):\n",
        "  global probability_list\n",
        "  global loss_list\n",
        "  global accuracy_list\n",
        "  global layer_list\n",
        "  global model_evolution_list\n",
        "  global neuron_total_list\n",
        "  global neuron_layer_list\n",
        "  global layer_list\n",
        "  global folderName\n",
        "  probability_list = []\n",
        "  loss_list = []\n",
        "  accuracy_list = []\n",
        "  layer_list = []\n",
        "  model_evolution_list = []\n",
        "  neuron_total_list = []\n",
        "  layer_list = []\n",
        "  neuron_layer_list = []\n",
        "  folderName = r'C:\\Projects\\n-spheres\\models\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '\\\\'\n",
        "  os.makedirs(folderName)\n",
        "  # FOR DEBUGGING PURPOSES ONLY/end\n",
        "  radius = calculate_radius(dimensions=dimensions, packing_ratio=pf, number_of_spheres=number_spheres, lower_volume_bound=0, upper_volume_bound=length_of_space)\n",
        "  beta = 0.15/nu\n",
        "  background = 1000\n",
        "  points_per_sphere = ((background * pf) / (number_spheres * (1 - pf))) / 2\n",
        "  data = get_spheres_binary(dimensions=dimensions, types_of_sphere=number_spheres, min_radius=radius, max_radius=radius, min_offset=radius, max_offset=length_of_space-radius, min_no_of_points=points_per_sphere, max_no_of_points=points_per_sphere, label_not_sphere=False, label_sphere=True, no_of_random_points=background, minimum=0, maximum=length_of_space)\n",
        "  sum_t = sum_f = 0\n",
        "  for i in data['label']:\n",
        "    if i == True:\n",
        "      sum_t += 1\n",
        "    else:\n",
        "      sum_f += 1\n",
        "  x = data.copy()\n",
        "  y = x.pop('label')\n",
        "  '''\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(y)\n",
        "  y_encoded = encoder.transform(y)\n",
        "  dummy_y = np_utils.to_categorical(y_encoded)\n",
        "  y = dummy_y\n",
        "  '''\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)\n",
        "  x_dev, x_test, y_dev, y_test = train_test_split(x_test, y_test, test_size=0.5)\n",
        "  '''\n",
        "  sc = MinMaxScaler()\n",
        "  x_train = sc.fit_transform(x_train)\n",
        "  x_dev = sc.transform(x_dev)\n",
        "  x_test = sc.transform(x_test)\n",
        "  '''\n",
        "  loss_list = []\n",
        "  acc_list = []\n",
        "\n",
        "  model = run_sim_for_loop_loss_included(x_train, y_train, x_dev, y_dev, 0.7, beta, nu, 512, 10, 2, 512, 500, loss_list, acc_list)\n",
        " \n",
        "  data_plot = pd.DataFrame({\"Loss\":loss_list, \"Runs\":[i for i in range(len(loss_list))]})\n",
        "  sns.lineplot(x = \"Runs\", y = \"Loss\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '0' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.png')\n",
        "  plt.close()\n",
        "  data_ploty = pd.DataFrame({\"accuracy\":accuracy_list, \"Runs\":[i for i in range(len(accuracy_list))]})\n",
        "  sns.lineplot(x = \"Runs\", y = \"accuracy\", data=data_ploty)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '1' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Accuracy\":accuracy_list, \"Layers\":layer_list})\n",
        "  sns.lineplot(x = \"Layers\", y = \"Accuracy\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '2' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Accuracy\":accuracy_list, \"Neurons\":neuron_total_list})\n",
        "  sns.lineplot(x = \"Neurons\", y = \"Accuracy\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '3' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.png')\n",
        "  plt.close()\n",
        "  model_evolution_list_hat = savgol_filter(model_evolution_list, 9, 1)\n",
        "  data_plot = pd.DataFrame({\"Loss_evo\":model_evolution_list_hat, \"Runs\":[i for i in range(len(model_evolution_list))]})\n",
        "  sns.lineplot(x = \"Runs\", y = \"Loss_evo\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '4' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Layers\":layer_list, \"Saves\":[i for i in range(len(layer_list))]})\n",
        "  sns.lineplot(x = \"Saves\", y = \"Layers\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '5' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Neurons\":neuron_total_list, \"Saves\":[i for i in range(len(neuron_total_list))]})\n",
        "  sns.lineplot(x = \"Saves\", y = \"Neurons\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '6' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.png')\n",
        "  plt.close()\n",
        "  x_test.to_csv(folder + 'test_datasets'+ '\\\\' + 'x' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.csv')\n",
        "  y_test.to_csv(folder + 'test_datasets'+ '\\\\' + 'y' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '.csv')\n",
        "  #os.makedirs()\n",
        "  #os.close(r'C:\\Projects\\n-spheres\\models')\n",
        "  #os.rename(r'C:\\Projects\\n-spheres\\models', r'C:\\Projects\\n-spheres\\models\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf))\n",
        "  #os.mkdir(r'C:\\Projects\\n-spheres\\models')\n",
        "  return\n",
        "def long_simulation_choose_start(dimensions, number_spheres, length_of_space, nu, pf, init_layers, init_starting_neurons, folder):\n",
        "  global probability_list\n",
        "  global loss_list\n",
        "  global accuracy_list\n",
        "  global layer_list\n",
        "  global model_evolution_list\n",
        "  global neuron_total_list\n",
        "  global neuron_layer_list\n",
        "  global layer_list\n",
        "  global folderName\n",
        "  probability_list = []\n",
        "  loss_list = []\n",
        "  accuracy_list = []\n",
        "  layer_list = []\n",
        "  model_evolution_list = []\n",
        "  neuron_total_list = []\n",
        "  layer_list = []\n",
        "  neuron_layer_list = []\n",
        "  folderName = r'C:\\Projects\\n-spheres\\models\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_layer_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '\\\\'\n",
        "  os.makedirs(folderName)\n",
        "  # FOR DEBUGGING PURPOSES ONLY/end\n",
        "  radius = calculate_radius(dimensions=dimensions, packing_ratio=pf, number_of_spheres=number_spheres, lower_volume_bound=0, upper_volume_bound=length_of_space)\n",
        "  beta = 0.1/nu\n",
        "  background = 1000\n",
        "  points_per_sphere = ((background * pf) / (number_spheres * (1 - pf))) / 2\n",
        "  data = get_spheres_binary(dimensions=dimensions, types_of_sphere=number_spheres, min_radius=radius, max_radius=radius, min_offset=radius, max_offset=length_of_space-radius, min_no_of_points=points_per_sphere, max_no_of_points=points_per_sphere, label_not_sphere=False, label_sphere=True, no_of_random_points=background, minimum=0, maximum=length_of_space)\n",
        "  sum_t = sum_f = 0\n",
        "  for i in data['label']:\n",
        "    if i == True:\n",
        "      sum_t += 1\n",
        "    else:\n",
        "      sum_f += 1\n",
        "  print(\"ratio \", sum_t/(sum_f+sum_t))\n",
        "  x = data.copy()\n",
        "  y = x.pop('label')\n",
        "  '''\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(y)\n",
        "  y_encoded = encoder.transform(y)\n",
        "  dummy_y = np_utils.to_categorical(y_encoded)\n",
        "  y = dummy_y\n",
        "  '''\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)\n",
        "  x_dev, x_test, y_dev, y_test = train_test_split(x_test, y_test, test_size=0.5)\n",
        "  '''\n",
        "  sc = MinMaxScaler()\n",
        "  x_train = sc.fit_transform(x_train)\n",
        "  x_dev = sc.transform(x_dev)\n",
        "  x_test = sc.transform(x_test)\n",
        "  '''\n",
        "  loss_list = []\n",
        "  acc_list = []\n",
        "\n",
        "  model = run_sim_for_loop_loss_included(x_train, y_train, x_dev, y_dev, 0.7, beta, nu, 512, init_layers, 1, init_starting_neurons, 400, loss_list, acc_list) #lowneuron=highneuron=1\n",
        "  data_plot = pd.DataFrame({\"Loss\":loss_list, \"Runs\":[i for i in range(len(loss_list))]})\n",
        "  sns.lineplot(x = \"Runs\", y = \"Loss\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '0' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.png')\n",
        "  plt.close()\n",
        "  data_ploty = pd.DataFrame({\"accuracy\":accuracy_list, \"Runs\":[i for i in range(len(accuracy_list))]})\n",
        "  sns.lineplot(x = \"Runs\", y = \"accuracy\", data=data_ploty)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '1' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Accuracy\":accuracy_list, \"Layers\":layer_list})\n",
        "  sns.lineplot(x = \"Layers\", y = \"Accuracy\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '2' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Accuracy\":accuracy_list, \"Neurons\":neuron_total_list})\n",
        "  sns.lineplot(x = \"Neurons\", y = \"Accuracy\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '3' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.png')\n",
        "  plt.close()\n",
        "  model_evolution_list_hat = savgol_filter(model_evolution_list, 9, 1)\n",
        "  data_plot = pd.DataFrame({\"Loss_evo\":model_evolution_list_hat, \"Runs\":[i for i in range(len(model_evolution_list))]})\n",
        "  sns.lineplot(x = \"Runs\", y = \"Loss_evo\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '4' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Layers\":layer_list, \"Saves\":[i for i in range(len(layer_list))]})\n",
        "  sns.lineplot(x = \"Saves\", y = \"Layers\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '5' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.png')\n",
        "  plt.close()\n",
        "  data_plot = pd.DataFrame({\"Neurons\":neuron_total_list, \"Saves\":[i for i in range(len(neuron_total_list))]})\n",
        "  sns.lineplot(x = \"Saves\", y = \"Neurons\", data=data_plot)\n",
        "  fig = plt.gcf()\n",
        "  plt.show()\n",
        "  fig.savefig(folder + '6' + '\\\\' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.png')\n",
        "  plt.close()\n",
        "  x_test.to_csv(folder + 'test_datasets'+ '\\\\' + 'x' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_layer_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.csv')\n",
        "  y_test.to_csv(folder + 'test_datasets'+ '\\\\' + 'y' + '_nu_' + str(nu) + '_pf_'+ str(pf) + '_start_layer_' + str(init_layers) + '_start_neuron' + str(init_starting_neurons) + '.csv')\n",
        "  return"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}