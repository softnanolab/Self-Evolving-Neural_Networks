{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import gamma\n",
        "from scipy import pi\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter"
      ],
      "metadata": {
        "id": "jqqP8_0EvrM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bK-60k2m6IP"
      },
      "outputs": [],
      "source": [
        "# Functions\n",
        "def add_neuron(model, chosen_layer: int):\n",
        "  model.layers[chosen_layer].units += 1\n",
        "  n_model = model_from_json(model.to_json())\n",
        "  return n_model\n",
        "def remove_neuron(model, chosen_layer: int):\n",
        "  model.layers[chosen_layer].units -= 1\n",
        "  n_model = model_from_json(model.to_json())\n",
        "  return n_model\n",
        "def add_layer(model, position, neurons):\n",
        "  '''\n",
        "  Adds a relu layer to a keras.Sequential model. Note: The implementation is a bit hacky and I am not satisfied with it.\n",
        "  Try to come up with other solutions\n",
        "  '''\n",
        "  layer_list = [layr.units for layr in model.layers]\n",
        "  #take the TensorFlow dimension, flips it, removes the None values, changes it into a tuple. This is done as in not to result in a multiple output shapes error (underdefined model)\n",
        "  input = tuple(filter(lambda item: item is not None, np.flip(model.input.shape)))\n",
        "  last_layer = layers.Dense(1, activation='sigmoid') # This is optimised for binary classification problems\n",
        "  layer_list.insert(position, neurons)\n",
        "  layer_list.pop()\n",
        "  n_model = keras.Sequential()\n",
        "  n_model.add(layers.Input(shape=input))\n",
        "  for neur in layer_list:\n",
        "    n_model.add(layers.Dense(units=neur, activation='relu'))\n",
        "  n_model.add(last_layer)\n",
        "  return n_model\n",
        "def remove_layer(model, position):\n",
        "  '''\n",
        "  works almost identically as add layer\n",
        "  '''\n",
        "  layer_list = [layr.units for layr in model.layers]\n",
        "  #take the TensorFlow dimension, flips it, removes the None values, changes it into a tuple. This is done as in not to result in a multiple output shapes error (underdefined model)\n",
        "  input = tuple(filter(lambda item: item is not None, np.flip(model.input.shape)))\n",
        "  last_layer = layers.Dense(1, activation='sigmoid') # This is optimised for binary classification problems\n",
        "  layer_list.pop(position)\n",
        "  layer_list.pop()\n",
        "  n_model = keras.Sequential()\n",
        "  n_model.add(layers.Input(shape=input))\n",
        "  for layr in layer_list:\n",
        "    n_model.add(layers.Dense(units=layr, activation='relu'))\n",
        "  n_model.add(last_layer)\n",
        "  return n_model  \n",
        "def remove_layer_experimental(model, position, neurons):\n",
        "  '''\n",
        "  I like this version of remove layer more. It's more versatile. However, I can see it causing plenty of bugs.\n",
        "  '''\n",
        "  layer_list = model.layers\n",
        "  #take the TensorFlow dimension, flips it, removes the None values, changes it into a tuple. This is done as in not to result in a multiple output shapes error (underdefined model)\n",
        "  input = tuple(filter(lambda item: item is not None, np.flip(model.input.shape)))\n",
        "  layer_list.pop(position)\n",
        "  n_model = keras.Sequential()\n",
        "  n_model.add(layers.Input(shape=input))\n",
        "  for layer in layer_list:\n",
        "    n_model.add(layer)\n",
        "  return n_model\n",
        "def calculate_neuron_acceptance_probability( NN:float, energy_N:float, energy_N_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = 1/(NN+1)\n",
        "  z = np.exp(-beta*nu)\n",
        "  exponent  = np.exp(-beta*(energy_N_1-energy_N))\n",
        "  probability = coeff * exponent * z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability \n",
        "def calculate_neuron_removal_probability(NN:float, energy_N:float, energy_N_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = NN\n",
        "  inverse_z = np.exp(beta*nu)\n",
        "  exponent  = np.exp(-beta*(energy_N-energy_N_1))\n",
        "  probability = coeff * exponent * inverse_z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability \n",
        "def calculate_layer_acceptance_probability(NL: float, M:float, energy_NL:float, energy_NL_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = (NL*M) / (NL+1)\n",
        "  z = np.exp(-beta*nu*M)\n",
        "  exponent = np.exp(-beta*(energy_NL_1-energy_NL))\n",
        "  probability = coeff * exponent * z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability\n",
        "def calculate_layer_removal_probability(NL: float, M:float, energy_NL:float, energy_NL_1, beta:float , nu:float) -> \"float\":\n",
        "  coeff = NL / (M*(NL-1))\n",
        "  inverse_z = np.exp(beta*nu*M)\n",
        "  exponent = np.exp(-beta*(energy_NL-energy_NL_1))\n",
        "  probability = coeff * exponent * inverse_z\n",
        "  if(probability > 1):\n",
        "    probability = 1\n",
        "  return probability\n",
        "def construct_model_binary_classification(input_shape: tuple, layer_list: list):\n",
        "  '''\n",
        "  Adapt the last layer to anything\n",
        "  '''\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "  for neurons in layer_list: \n",
        "    model.add(keras.layers.Dense(neurons, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "def calculate_GC(model, loss:  float, nu: float) -> \"float\":  \n",
        "  return loss + nu*model.count_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xecdAZy4yIe"
      },
      "outputs": [],
      "source": [
        "# run simulation functions\n",
        "# FOR DEBUGGING PURPOSES ONLY\n",
        "probability_list = []\n",
        "loss_list = []\n",
        "accuracy_list = []\n",
        "layer_list = []\n",
        "model_evolution_list = []\n",
        "neuron_total_list = []\n",
        "layer_list = []\n",
        "neuron_layer_list = []\n",
        "# FOR DEBUGGING PURPOSES ONLY/end\n",
        "#REMEMBER TO ADD ANOTATIONS TO THE NEW SAVING METHODS \n",
        "def compare_losses(model, saved_model_labels, x_test, y_test, repo):\n",
        "  #could probably be made more efficient by also saving the losses?\n",
        "  model_loss, model_accuracy = model.evaluate(x_test, y_test)\n",
        "  if(len(saved_model_labels)<5):\n",
        "    repo += str(time.time())\n",
        "    model.save(repo)\n",
        "    return\n",
        "  for model_challenged_label in saved_model_labels:\n",
        "    model_challenged = load_model(repo + str(model_challenged_label))\n",
        "    #what is more efficient? writing a new code as a function or just rolling with it? I mean in all honesty the function would have to be changed with every new configuration so...same thing?\n",
        "    model_challenged_loss, model_challenged_accuracy = model_challenged.evaluate(x_test, y_test)\n",
        "    if model_loss < model_challenged_loss :\n",
        "      shutil.rmtree(repo + model_challenged_label)\n",
        "      repo += str(time.time())\n",
        "      model.save(repo)\n",
        "      #add removal of worst model, somehow\n",
        "\n",
        "      break\n",
        "  return\n",
        "def save_best_model(model, x_test, y_test, repo):\n",
        "  saved_model_labels = []\n",
        "  for filename in os.listdir(repo):\n",
        "    saved_model_labels.append(filename)\n",
        "  compare_losses(model, saved_model_labels, x_test, y_test, repo)\n",
        "  return\n",
        "def save_model(model, x_train, y_train, x_test, y_test):\n",
        "  #ADDED THE DATA FOR DEBUGING\n",
        "  #ADDED THIS FOR DEBUGGING AND ANALISING DATA\n",
        "  _layers = model.layers\n",
        "  _layers_neurons = [_layer.units for _layer in _layers]\n",
        "  _neurons_with_layers = list(zip(_layers_neurons, _layers))\n",
        "  neurons = sum(_layers_neurons)\n",
        "  layer_list.append(len(_layers))\n",
        "  neuron_total_list.append(neurons)\n",
        "  neuron_layer_list.append(_neurons_with_layers)\n",
        "  #END THINGS USED FOR PLOTTING\n",
        "  name = folderName\n",
        "  save_best_model(model, x_test, y_test, name)\n",
        "  name += str(time.time())\n",
        "  return\n",
        "def run_model(model, x_train, y_train, x_test, y_test):\n",
        "  #REMEMBER TO MAKE BOTH IDENTICAL AT ALL TIMES\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "  model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=15,verbose=0)\n",
        "  loss , accuracy = model.evaluate(x_test, y_test)\n",
        "  return loss\n",
        "def run_model_DEBUG(model, x_train, y_train, x_test, y_test):\n",
        "  #REMEMBER TO MAKE BOTH IDENTICAL AT ALL TIMES\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "  model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=15,verbose=0)\n",
        "  loss , accuracy = model.evaluate(x_test, y_test)\n",
        "  return loss, accuracy\n",
        "\n",
        "def run_sim_neuron(model, loss, x_train, y_train, x_test, y_test, beta, nu):\n",
        "  '''\n",
        "  function that runs move 1. loss was the term chosen for the energy of the system before the move, energy was chosen for the energy fo the system after the move.\n",
        "  even if they are technically the same in terms of units (both losses/energy)\n",
        "  this was chosen to avoid confusion between the two\n",
        "  '''\n",
        "  #brute force functions to solve the neuron removal problem\n",
        "  check_neurons_list = [lay.units <= 1 for lay in model.layers[1:-1]]\n",
        "  if all(check_neurons_list) == True:\n",
        "    return model\n",
        "  while True: \n",
        "    if(len(model.layers) == 2): \n",
        "      NN = 1\n",
        "    else:\n",
        "      layer = np.random.randint(low=1, high=len(model.layers)-1)\n",
        "    NN = model.layers[layer].units\n",
        "    if NN > 1: \n",
        "      break\n",
        "  #END BRUTE FORCE\n",
        "  add_neuron_prob = np.random.choice([True, False])\n",
        "  if add_neuron_prob == True: \n",
        "    new_model = add_neuron(model, layer)\n",
        "  else:\n",
        "    new_model = remove_neuron(model, layer)\n",
        "  #calculate model loss \n",
        "  energy, accuracy = run_model_DEBUG(new_model, x_train, y_train, x_test, y_test) # ADDED ACCURACY FOR DEBUGGING\n",
        "  #run acceptance\n",
        "  if(add_neuron_prob == True):\n",
        "    probability = calculate_neuron_acceptance_probability(NN, loss, energy, beta, nu)\n",
        "  else:\n",
        "    probability = calculate_neuron_removal_probability(NN-1, energy, loss, beta, nu)\n",
        "    # it is assumed that for this NN+1(i) is the model before the operation, and NN(i) is the model after the operation, hence the NN-1, and the reversed orders of energy and loss.\n",
        "  if np.random.rand() < probability:\n",
        "    probability_list.append(probability) #FOR DEBUGGING ONLY \n",
        "    #just for trial, we save every model we ever make for this time\n",
        "    save_model(new_model, x_train, y_train, x_test, y_test)\n",
        "    loss_list.append(energy)\n",
        "    accuracy_list.append(accuracy)\n",
        "    model_evolution_list.append(energy)\n",
        "\n",
        "    return new_model\n",
        "  else:\n",
        "    model_evolution_list.append(loss) #FOR DEBUGGING, both\n",
        "    return model\n",
        "\n",
        "def run_sim_layer(model, loss, x_train, y_train, x_test, y_test, beta, nu, M):\n",
        "  '''\n",
        "  function that runs move 1. loss was the term chosen for the energy of the system before the move, energy was chosen for the energy fo the system after the move.\n",
        "  even if they are technically the same in terms of units (both losses/energy)\n",
        "  this was chosen to avoid confusion between the two\n",
        "  '''\n",
        "  neurons = np.random.randint(low=1, high=M)\n",
        "  add_layer_prob = np.random.choice([True, False])\n",
        "  if add_layer_prob == True: \n",
        "    layer = np.random.randint(low=1, high=len(model.layers)-1)\n",
        "    new_model = add_layer(model, layer, neurons)\n",
        "  else:\n",
        "    #brute force way\n",
        "    if(len(model.layers)-2 <= 1):\n",
        "      return model\n",
        "    layer = np.random.randint(low=1, high=len(model.layers)-2)\n",
        "    removed_neurons = model.layers[layer].units\n",
        "    new_model = remove_layer(model, layer)\n",
        "  #calculate model loss \n",
        "  NL = len(new_model.layers)\n",
        "  energy, accuracy = run_model_DEBUG(new_model, x_train, y_train, x_test, y_test) #ADDED ACCURACY FOR DEBUGGING\n",
        "  #run acceptance\n",
        "  if(add_layer_prob == True):\n",
        "    probability = calculate_layer_acceptance_probability(NL, neurons, loss, energy, beta, nu)\n",
        "  else:\n",
        "    probability = calculate_layer_removal_probability(NL, removed_neurons, energy, loss, beta, nu)\n",
        "    # it is assumed that for this NN+1(i) is the model before the operation, and NN(i) is the model after the operation, hence the NN-1, and the reversed orders of energy and loss.\n",
        "  if np.random.rand() < probability:\n",
        "    probability_list.append(probability) #FOR DEBUGGING PURPOSES ONLY\n",
        "    #just for trial, we save every model we ever make for this time\n",
        "    save_model(new_model, x_train, y_train, x_test, y_test)\n",
        "    loss_list.append(energy)\n",
        "    accuracy_list.append(accuracy)\n",
        "    model_evolution_list.append(energy)\n",
        "    return new_model\n",
        "  else:\n",
        "    model_evolution_list.append(loss)\n",
        "    return model\n",
        "\n",
        "def run_sim_step(model, x_train, y_train, x_test, y_test, f, beta, nu, M):\n",
        "  loss = run_model(model, x_train, y_train, x_test, y_test)\n",
        "  if np.random.rand() < f:\n",
        "    model = run_sim_neuron(model, loss, x_train, y_train, x_test, y_test, beta, nu)\n",
        "  else:\n",
        "    model = run_sim_layer(model, loss, x_train, y_train, x_test, y_test, beta, nu, M)\n",
        "  return model \n",
        "def run_sim_for_loop(x_train, y_train, x_test, y_test, f, beta, nu, M, number_of_starting_layers, low_neuron, high_neuron, iterations):\n",
        "  starting_layers = np.random.randint(low=2, high=number_of_starting_layers)\n",
        "  if(low_neuron == high_neuron):\n",
        "    model_layers = [low_neuron for i in range(starting_layers)]\n",
        "  else:\n",
        "    model_layers = np.random.randint(low=low_neuron, high=high_neuron, size=starting_layers)\n",
        "  model = construct_model_binary_classification(x_train.shape[1], model_layers)\n",
        "  loss = run_model(model, x_train, y_train, x_test, y_test)\n",
        "  save_model(model, x_train, y_train, x_test, y_test)\n",
        "  energy, accuracy = run_model_DEBUG(model, x_train, y_train, x_test, y_test)\n",
        "  accuracy_list.append(accuracy)\n",
        "  model_evolution_list.append(energy)\n",
        "  for i in range(iterations):\n",
        "    model = run_sim_step(model, x_train, y_train, x_test, y_test, f, beta, nu, M)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTOQPhsy6mR1"
      },
      "outputs": [],
      "source": [
        "# for debugging purposes \n",
        "def run_sim_for_loop_loss_included(x_train, y_train, x_test, y_test, f, beta, nu, M, number_of_starting_layers, low_neuron, high_neuron, iterations, loss_list=[], mae_list=[]):\n",
        "  starting_layers = np.random.randint(low=2, high=number_of_starting_layers)\n",
        "  if(low_neuron == high_neuron):\n",
        "    model_layers = [low_neuron for i in range(starting_layers)]\n",
        "  else:\n",
        "    model_layers = np.random.randint(low=low_neuron, high=high_neuron, size=starting_layers)\n",
        "  model = construct_model_binary_classification(x_train.shape[1], model_layers)\n",
        "  loss = run_model(model, x_train, y_train, x_test, y_test)\n",
        "  save_model(model, x_train, y_train, x_test, y_test)\n",
        "  loss_list.append(loss)\n",
        "  energy, accuracy = run_model_DEBUG(model, x_train, y_train, x_test, y_test)\n",
        "  accuracy_list.append(accuracy)\n",
        "  model_evolution_list.append(energy)\n",
        "  for i in range(iterations):\n",
        "    model = run_sim_step(model, x_train, y_train, x_test, y_test, f, beta, nu, M)\n",
        "    #loss, mae = model.evaluate(x_test, y_test)\n",
        "    #loss_list.append(loss)\n",
        "    #mae_list.append(mae)\n",
        "  return model # loss_list, mae_list"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}